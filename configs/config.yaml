mode: dev # Set to dev when testing, otherwise set to prod

agent:
  type: "seq2seq" # causal
  name: "google/flan-t5-small" # HuggingFaceTB/SmolLM2-135M
  device: cpu

model_params:
  max_new_tokens: 256
  temperature: 0.7
  do_sample: true
  top_p: 0.9
  num_return_sequences: 1
  min_new_tokens: 10

experts:
  num_experts: 2
  debate_rounds: 3

lora:
  enabled: true
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  bias: "none"

training:
  output_dir: "./results"
  eval_strategy: "steps"
  save_strategy: "no"
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  eval_accumulation_steps: 4
  eval_steps: 200
  max_steps: 200
  logging_steps: 20
  learning_rate: 1e-5
  weight_decay: 0.01

data:
  data_cache_dir: "./data/cache"
  category: "math"
  name: "gsm8k"
  split: ["train[:10%]+validation[:5%]+test[:5%]"]